<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Selective Underfitting: a new lens to understand how diffusion models generalize, when they underfit, and why popular training recipes like DiT/REPA work.">
  <meta name="keywords" content="diffusion models, selective underfitting, score matching, REPA, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Selective Underfitting in Diffusion Models</title>
  <link rel="canonical" href="https://selective-underfitting.github.io/"/>

  <!-- MathJax for LaTeX rendering (optional) -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    window.MathJax = { tex: { inlineMath: [['$', '$'], ['\(', '\)']], displayMath: [['$$','$$'], ['\[','\]']] } };
  </script>

  <!-- (Optional) Analytics – fill in your ID or remove -->
  <script>
    // window.dataLayer = window.dataLayer || [];
    // function gtag(){dataLayer.push(arguments);} gtag('js', new Date());
    // gtag('config', 'G-XXXXXXXXXX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma / Icons / Local styles (match FlexMDM structure) -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <style>
    /* Minimal extras if ./static/css/index.css is empty */
    .publication-title { letter-spacing: -0.02em; }
    .hero.is-primary { background: linear-gradient(180deg,#0f1221,#14182c); }
    .teaser img { border-radius: 10px; box-shadow: 0 12px 32px rgba(0,0,0,.25); }
    .fancy-table { width: 100%; border-collapse: collapse; }
    .fancy-table th,.fancy-table td{ border:1px solid #e6e6e6; padding:8px 10px; }
    .fancy-table th{ background:#f6f7fb; }
    .section .subtitle strong{ font-weight:700; }
    .tagline { color:#6b7280; margin-top:.25rem; }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- HERO -->
<section class="hero is-white">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Selective Underfitting in Diffusion Models</h1>
          <p class="tagline">A new rule of understandig diffusion models for how diffusion models learn, generalize, and scale.</p>

          <div class="is-size-5 publication-authors" style="margin-top:10px">
            <span class="author-block"><a href="https://kiwhan.dev/" target="_blank">Kiwhan Song</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block"><a href="https://jaeyeonkim01.github.io/" target="_blank">Jaeyeon Kim</a><sup>*</sup><sup>2</sup>,</span>
            <span class="author-block"><a href="https://sitanchen.com" target="_blank">Sitan Chen</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://yilundu.github.io" target="_blank">Yilun Du</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://shamulent.github.io" target="_blank">Sham Kakade</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a><sup>1</sup></span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>1</sup>MIT</span>
            <span class="author-block"><sup>2</sup>Harvard</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.01378" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://typefully.com/t/xCWbsZ1" class="external-link button is-normal is-rounded is-light" target="_blank" rel="noopener">
                  <span class="icon"><i class="fab fa-x-twitter"></i></span>
                  <span>Thread</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- INTRO / SUMMARY FROM THREAD -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p class="no-mathjax">
            Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. Despite their success, they face an apparent paradox: if training were perfect, they would learn the empirical score function–the score with respect to the finite training data–and simply replicate the training data at inference, never generating novel samples.
          </p>
          <p class="no-mathjax">
            Recent work has sought to resolve this paradox by arguing that diffusion models “globally” underfit the empirical score and suggest that inductive bias or smoothness of neural networks are the reason for this, including (Kamb &amp; Ganguli 2024; Niedoba et al. 2024; Scarvelis et al. 2023; Pidstrigach 2022; Yoon et al. 2023; Yi et al. 2023). Instead, we argue that understanding diffusion models requires us to investigate not just how they underfit the empirical score, but where they underfit. We show that diffusion models do not underfit in a certain region of the space and selectively underfit beyond it. This provides a novel lens for explaining diffusion models’ behavior.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- SELECTIVE UNDERFITTING SECTION -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Selective Underfitting</h2>
    <div class="content">
      <ul>
        <p class="no-mathjax">
          We first distinguish two regions of the space: <strong>supervision region</strong> and <strong>extrapolation region</strong>. Error between the learned score and the empirical score decreases in the supervision region (X underfitting, blue line) but increases in the extrapolation region (O underfitting, red line) as models improve with better FID scores. These two regions can be further characterized as follows:        
        </p>
        <ul class="no-mathjax">
          <li><strong>Supervision region</strong>: where the noisy training samples concentrated with a high probability. A model is supervised to learn the empirical score in this region (blue shells).</li>
          <li><strong>Extrapolation region</strong>: where the model is actually queried during inference. A model is not supervised to learn the empirical score in this region (red region).</li>
          <li>In supervision region, the learned and empirical scores are <strong>sufficiently close</strong> that the model reproduces the training images when sampling starts from there.</li> 
        </ul>
        <p class="no-mathjax">
          Therefore, we claim that the right way to understand diffusion models is to think of them as operating by <strong>(1)</strong> approximating the empirical score within the supervision region and <strong>(2)</strong> extrapolating it to the extrapolation region, which is ultimately used at inference time.
        </p>
        <img src="./static/images/selective.jpg" alt="Selective underfitting"/>
      </ul>
    </div>
  </div>
</section>

<!-- GENERALIZATION SECTION -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Generalization</h2>
    <div class="content">
      <ul class="no-mathjax">
        <li>This perspective is crucial for understanding diffusion model generalization: the size of the supervision region strongly affects a model's ability to generalize. Enlarging this region degrades generalization—a trend that persists regardless of architecture, e.g., SiT, U-Net.</li>
        <li>This behavior cannot be explained by previous works, which focus on how inductive bias shapes underfitting of the empirical score. Our perspective adds a complementary dimension: to fully understand diffusion model generalization, one must consider where the model underfits, not only how.</li>
      </ul>
      <p class="has-text-centered">
        <img src="./static/images/generalization.jpg" alt="Generalization"/>
      </p>
    </div>
  </div>
</section>

<!-- SCALING LAW -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Scaling Law</h2>
    <div class="content">
      <p>
        Next, we decompose a model's FID into two components:
        <ul class="no-mathjax">
          <li> How well it approximates the empirical score in the supervision region. This translates to the training loss.</li>
          <li> How this approximation translates to generative performance (training loss → FID), i.e., extrapolation efficiency.
        This provides a quantative and comprehensive lens to understand why recent diffusion training recipes work well.
      </p>
    </div>
    <div class="columns is-vcentered">
      <div class="column is-half">
        <img src="./static/images/scaling_law.jpg" alt="Scaling law"/>
      </div>
      <div class="column is-half">
        <div class="content">
          <ul class="no-mathjax">
            <li><strong>RePA</strong> barely alters the model’s behavior in the supervision region but greatly changes its extrapolation behavior—endowing it with better extrapolation efficiency.</li>
            <li><strong>Transformers (DiT)</strong> trail U-Nets in extrapolation efficiency but excel in FLOPs→supervision loss. Combining both factors, transformers achieve higher overall compute efficiency (FLOPs → FIDs).</li>
            <li>We further argue that recent diffusion training methods, such as <strong>RePA</strong> and <strong>ReDi</strong>, align the score network’s output with the learned representation, improving generative performance by better handling underfitting (extrapolation) regions.</li>
          </ul>
        </div>
      </div>
  </div>
</section>

<!-- TAKEAWAYS -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Takeaways</h2>
    <div class="content">
      <ol>
        <li><strong>Diffusion models don’t underfit everywhere, but selectively in distinct regions of data space.</strong></li>
        <li><strong>The relative sizes of these regions affect generalization.</strong></li>
        <li><strong>Decomposing FID into this interaction reveals why recipes like RePA and DiT/SiT work well.</strong></li>
      </ol>
      <p>
        Last but not least, our lens of selective underfitting opens the door to new questions. Principled question is on how the score is shaped in extrapolation regions. The mechanism for how a model extrapolates toward a favorable score that generates good perceptual samples is still questionable. Further practical question is how to design training recipes that better balance supervision and extrapolation.
      </p>
    </div>
  </div>
</section>

<!-- BIBTEX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{song2025selective,
  title   = {Selective Underfitting in Diffusion Models},
  author  = {Song, Kiwhan and Kim, Jaeyeon and Chen, Sitan and Du, Yilun and Kakade, Sham and Sitzmann, Vincent},
  journal = {arXiv preprint arXiv:2510.01378},
  year    = {2025}
}</code></pre>
  </div>
</section>

<!-- FOOTER -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#" class="external-link" aria-disabled="true">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
          <p>This webpage is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and the FlexMDM site template.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>